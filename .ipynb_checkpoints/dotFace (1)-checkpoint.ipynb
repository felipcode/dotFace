{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aec859b-d8de-4a4d-bc93-bbd7a141da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import cvzone\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "#from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35fa21-dcae-428d-95cb-a55699541e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class FaceRecognitionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "\n",
    "        \n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                self.file_paths.append(os.path.join(class_dir, file_name))\n",
    "                self.labels.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image    = cv2.imread(img_path)\n",
    "        image    = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image    = Image.fromarray(image)\n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image)\n",
    "        plt.imshow(np.transpose(np.array(img), (1, 2, 0)))\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d9983-1086-4a4d-8c4e-116711ffdd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((160, 160)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset = FaceRecognitionDataset('./FACES', transform = transform)\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd9c3ea-8843-45a0-a779-6b37430f5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276569a6-15ec-4917-acb4-570e4d865dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If required, create a face detection pipeline using MTCNN:\n",
    "detector = MTCNN(keep_all = True, device = device, margin = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb1889-b69d-46d2-bff9-fa1e3e4ae7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Faces\n",
    "img = cv2.imread(img_p)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "pil_img = Image.fromarray(img)\n",
    "\n",
    "boxes, _ = detector.detect(pil_img)\n",
    "# Calculate embedding (unsqueeze to add batch dimension)\n",
    "faces = []\n",
    "for i in boxes:\n",
    "    x, y, x1, y1 = i\n",
    "    faces.append(img[int(y):int(y1), int(x):int(x1)])\n",
    "\n",
    "ctt = len(faces)\n",
    "ct = 0\n",
    "# SHOW DETECTED FACES\n",
    "for i in faces:\n",
    "    pil = Image.fromarray(i)\n",
    "    plt.figure(ct)\n",
    "    ct+=1\n",
    "    plt.imshow(pil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f8c5e-cd05-41d7-9f4d-c61156940ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefd48b-af83-477b-8dd9-2c7a3e431e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "if( boxes is not None ):\n",
    "    for i, box in enumerate(boxes):\n",
    "        x, y, w, h = box\n",
    "        ax.text(x, y, str(i+1), fontsize = 12, color='cyan')\n",
    "        rect = plt.Rectangle((x, y), w-x, h-y, fill= False, color='magenta', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "    print(f\"Number of faces: {len(boxes)}\")\n",
    "else:\n",
    "    print(\"No faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46820c-e814-4a8b-a51c-df82d9e192e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print( \"Error: Could not open camera\" )\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame\")\n",
    "        break\n",
    "    img_t = torchvision.transforms.ToTensor()\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(img)\n",
    "\n",
    "    boxes, _ = detector.detect(pil_img)\n",
    "\n",
    "    if( boxes is not None ):\n",
    "        for i, box in enumerate(boxes):\n",
    "            x, y, w, h = box\n",
    "            #ax.text(x, y, str(i+1), fontsize = 12, color='cyan')\n",
    "            #rect = plt.Rectangle((x, y), w-x, h-y, fill= False, color='magenta', linewidth=2)\n",
    "            #ax.add_patch(rect)\n",
    "            x, y, w, h = int(x+1), int(y+1), int(w-1), int(h-1)\n",
    "            img_face = img[y: h, x: w]\n",
    "            if len(img_face) > 2:\n",
    "                cv2.imshow('face', img_face)\n",
    "            #break\n",
    "            print(x, y, w, h)\n",
    "            cv2.rectangle(frame, (int(x), int(y)), (int(w), int(h)), (0,0,255), 2)\n",
    "          \n",
    "            cvzone.putTextRect(frame, str(i+1), [int(x), int(y+10)], scale=1, thickness =1)\n",
    "            print(f\"Number of faces: {len(boxes)}\")\n",
    "    else:\n",
    "        print(\"No faces\")\n",
    "    \n",
    "    \n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    if( cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163e429-7b23-4074-b67b-9c39f37bf7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07b4156-b50d-4651-8988-065ffd67f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import cvzone\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ----------- MODELS ----------- #\n",
    "resnet = InceptionResnetV1(pretrained='vggface2', classify=True).eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "detector = MTCNN(keep_all = True, device = device, margin = 10)\n",
    "#---------------------------------#\n",
    "\n",
    "def get_face_embedding(image):\n",
    "    # Convert image from BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(image_rgb)\n",
    "\n",
    "    # Detect face and get bounding box\n",
    "    boxes, _ = detector.detect(pil_image)\n",
    "\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    # Extract face from image\n",
    "    x, y, w, h = boxes[0].astype(int)\n",
    "    face_image = pil_image.crop((x, y, w, h))\n",
    "\n",
    "    # Resize face image to match model input size\n",
    "    face_tensor = transform(face_image).unsqueeze(0).to(device)\n",
    "    resnet.to(device)\n",
    "    # Get face embedding using InceptionResnetV1 model\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        embedding = resnet(face_tensor)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# Define known faces and their embeddings (you need to populate this with your own known faces)\n",
    "known_faces = {\n",
    "    \"felps\": get_face_embedding((cv2.imread(\"DATA_TRAIN/Felipe/img_10.jpg\"))),\n",
    "    \"INe\": get_face_embedding(cv2.imread(\"DATA_TRAIN/Jaca/img_5.jpg\")),\n",
    "#    \"mateus\": get_face_embedding(cv2.imread(\"FACES/Mateus/img_3.jpg\")),\n",
    "    # Add more known faces as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151cebc1-461a-4a80-a8b9-5170aff92f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename = \"my_checkpoint_xx.pth.tar\"):\n",
    "    print(\"=> Saving Checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92482809-c8f6-4939-97bf-4b9197f5ef45",
   "metadata": {},
   "source": [
    "# One code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2657403b-66eb-4993-b99e-c17777922fc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149.51619\n",
      "felps  Detected!\n",
      "152.98122\n",
      "felps  Detected!\n",
      "180.8203\n",
      "170.30765\n",
      "INe  Detected!\n",
      "180.8203\n",
      "170.30765\n",
      "INe  Detected!\n",
      "175.68024\n",
      "159.12088\n",
      "INe  Detected!\n",
      "109.06496\n",
      "felps  Detected!\n",
      "101.07203\n",
      "felps  Detected!\n",
      "106.03563\n",
      "felps  Detected!\n",
      "90.15307\n",
      "felps  Detected!\n",
      "93.797844\n",
      "felps  Detected!\n",
      "92.842354\n",
      "felps  Detected!\n",
      "97.17439\n",
      "felps  Detected!\n",
      "79.11109\n",
      "felps  Detected!\n",
      "Error: Failed to capture frame\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import cvzone\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ------ COLECT DATA FROM CAMERA ------ #\n",
    "count = 0                         # counter for filename\n",
    "save = 1                          # save flag\n",
    "outFolder = './DATA_TRAIN/Jaca'       # output Foler\n",
    "blurThreshold = 200               # Blur for Capture of Photos (300, 400)\n",
    "#---------------------------------------#\n",
    "\n",
    "\n",
    "# ----------- MODELS ----------- #\n",
    "resnet = InceptionResnetV1(pretrained='vggface2', classify=True).eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "detector = MTCNN(keep_all = True, device = device, margin = 10)\n",
    "detector.to(device)\n",
    "live = torchvision.models.mobilenet_v2()\n",
    "num_classes = 2\n",
    "live.classifier[1] = torch.nn.Linear(live.last_channel, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# 4. Optimize your model\n",
    "optimizer = torch.optim.SGD(live.parameters(), lr=0.001, momentum=0.9)\n",
    "#load_checkpoint('./liveliness_UP.pth.tar', live)\n",
    "live.eval()\n",
    "#---------------------------------#\n",
    "\n",
    "resnet.to(device)\n",
    "detector.to(device)\n",
    "live.to(device)\n",
    "\n",
    "\n",
    "# ------ Capture From Camera ----- #\n",
    "cap = cv2.VideoCapture(0)\n",
    "#cap = cv2.VideoCapture('rtsp://192.168.3.22/Streaming/Channels/101')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print( \"Error: Could not open camera\" )\n",
    "#----------------------------------#\n",
    "\n",
    "# ------- Parameters for face Recon ------- #\n",
    "threshold = 190\n",
    "minDist = 99999\n",
    "#-------------------------------------------#\n",
    "\n",
    "# ---------- VARIABLES OF FRAME ----------#\n",
    "who = 'unknown'\n",
    "text = 'unknown'\n",
    "person = 'unknown'\n",
    "list_person = ['unknown' for _ in range(10)]\n",
    "index=0\n",
    "identified = []\n",
    "update_flag = np.zeros(10)\n",
    "size =0\n",
    "last_size =0\n",
    "alive = ['fake' for _ in range(10)]\n",
    "#-----------------------------------------#\n",
    "\n",
    "size = 0\n",
    "last_size = 0\n",
    "while True:\n",
    "    # GET FRAMES\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture frame\")\n",
    "        break\n",
    "\n",
    "    # IMAGE CONVERSION\n",
    "    img_t = torchvision.transforms.ToTensor()\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(img)\n",
    "\n",
    "    # DETECT FACES IN FRAME\n",
    "    boxes, probs = detector.detect(pil_img)\n",
    "    who = 'unknown'    # Set variable incertanty\n",
    "    # ITERATE OVER THE DETECTED FACES\n",
    "    if( boxes is not None ):\n",
    "        #size = len(boxes)\n",
    "        #if(size < last_size):\n",
    "        #    list_person = ['unknown' for _ in range(10)]\n",
    "        #    identified = []\n",
    "        #    text = 'unknown'\n",
    "        #last_size = size\n",
    "        if (len(boxes) == 1):\n",
    "            list_person[1:] = ['unknown' for _ in range(9)]\n",
    "        for i, box in enumerate(boxes):\n",
    "            if probs[i] > 0.94:                                     # ACCEPTANCE LEVEL\n",
    "                x, y, w, h = box                                    # BOUNDING BOXES COORDINATES\n",
    "                x, y, w, h = int(x+1), int(y+1), int(w-1), int(h-1) # CONVERT TO INT\n",
    "                img_face = img[y: h, x: w]                          # CROP IMAGE\n",
    "                \n",
    "                # ACCEPT ONLY FACES WITH A CONSIDERABLE SIZE\n",
    "                if (img_face.size) > 50000 and len(img_face[0]) > 2:\n",
    "                    img_face = cv2.cvtColor(img_face, cv2.COLOR_BGR2RGB)\n",
    "                    pil_crop = Image.fromarray(img_face)\n",
    "                    # Ensure the input tensor has the correct shape\n",
    "                    img_tensor = transform(pil_crop)\n",
    "                    \n",
    "                    # Convert the tensor to the expected data type (torch.ByteTensor)\n",
    "                    img_tensor = img_tensor.unsqueeze(0)\n",
    "                    \n",
    "                    # Pass the tensor to the model\n",
    "                    liveliness = live(img_tensor.to(device))\n",
    "                    _, predicted = torch.max(liveliness, 1)  # Get the index of the class with the highest probability\n",
    "                    # Print the output\n",
    "                    #print(predicted)\n",
    "                    \n",
    "                    cv2.imshow('face', img_face)\n",
    "                    \n",
    "                    # Blur calc For saving and recon\n",
    "                    blurValue = cv2.Laplacian(img_face, cv2.CV_64F).var()\n",
    "                    # Embeddings (face-info)\n",
    "                    detected_embedding = get_face_embedding(img_face)\n",
    "                    \n",
    "                    if (detected_embedding is not None):\n",
    "                        ct =0\n",
    "                        minDist = 99999\n",
    "                        # ITERATE OVER THE KNOWN FACES\n",
    "                        for name, known_emb in known_faces.items():\n",
    "                            if known_emb is not None:\n",
    "                                distance = np.linalg.norm(detected_embedding.cpu() - known_emb.cpu())\n",
    "                                # print(distance, ' [', ct, ']')\n",
    "                                # Determine the face that te sum of all distances is most similar (accept a level of Blur)\n",
    "                                if (blurValue > blurThreshold) and (distance < threshold) and (distance < minDist):\n",
    "                                    minDist = distance\n",
    "                                    who = name\n",
    "                                    update_flag[i] = 1\n",
    "                                    print(distance)\n",
    "                            ct+=1\n",
    "                    # SAVE FLAG FOR COLECTING TRAINIG DATA\n",
    "                    if save:\n",
    "                        if blurValue > blurThreshold:\n",
    "                            cv2.imwrite(f\"{outFolder}/img_{count}.jpg\" , img_face)\n",
    "                            count+=1\n",
    "\n",
    "                # Update the vector of names of detected person\n",
    "                if update_flag[i]:\n",
    "                    person = who\n",
    "                    update_flag[i] = 0\n",
    "                    print(person, \" Detected!\")\n",
    "                    #ss = person + f\" ID: {i}\"\n",
    "                    list_person[i] = person          # Update on the index of the face \n",
    "                    \n",
    "                    \n",
    "                  \n",
    "                # BBOX face\n",
    "                cv2.rectangle(frame, (int(x), int(y)), (int(w), int(h)), (0,0,255), 2)\n",
    "                \n",
    "                # Show if face is alive or NOT\n",
    "                for j in range(len(alive)):\n",
    "                    if j == i:\n",
    "                        if predicted == 1:\n",
    "                            alive[i] = ' REAL'\n",
    "                        else:\n",
    "                            alive[i] = ' fake'\n",
    "                        \n",
    "                #text += f\" ID: {i}\" #+ f\" Blur: {blurValue}\"\n",
    "                text = str(list_person[i]) + alive[i] + ' ' + str(i)\n",
    "                cvzone.putTextRect(frame, text, [int(x), int(y+10)], scale=1, thickness =1)\n",
    "\n",
    "                #print(f\"Number of faces: {len(boxes)}\")\n",
    "    else:\n",
    "        #print(\"No faces\")\n",
    "        list_person = ['unknown' for _ in range(10)]\n",
    "        identified = []\n",
    "        text = 'unknown'\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    if( cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fa86df64-7848-47f9-a3fd-8d619ce63982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_flag = np.zeros(10)\n",
    "update_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f50410-f6d6-4bf5-a8e5-ae1a5edc7fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
